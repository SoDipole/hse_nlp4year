{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск коллокаций для оценочных слов\n",
    "С помощью nltk.collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from preprocessing import tokenize\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка корпуса рецензий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SentiRuEval_rest_train.xml\", encoding = \"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"lxml\")\n",
    "\n",
    "reviews = []\n",
    "for review in soup.find_all(\"review\"):\n",
    "    text = review.find(\"text\").text\n",
    "    reviews.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19034\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текстов\n",
    "Токенизация и стопслова. Можно добавить лемматизацию (но пока без неё из-за времени работы на большом объёме корпуса)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(raw_text):\n",
    "    clean_text = re.sub('\\W+', ' ', raw_text) # \\W = [^a-zA-Z0-9_]\n",
    "    clean_text = re.sub('[0-9]+', ' ', clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tokenized = []\n",
    "for review in reviews:\n",
    "    words = preprocessing(review).lower().split()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "#         p = morph.parse(word)[0]\n",
    "#         lemmas.append(p.normal_form)\n",
    "        if word not in stop_words and len(word) > 1:\n",
    "            lemmas.append(word)\n",
    "    reviews_tokenized.append(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['пускай', 'обижается', 'наш', 'прославленный', 'защитник', 'франкофон', 'монреаль', 'канадиенс', 'maxime', 'некоторой', 'опаской', 'относился', 'этому', 'народу', 'народу', 'способному', 'таким', 'благоговением', 'доводить', 'цирроза', 'пернатую', 'живность', 'заставлять', 'специальных', 'поисковых', 'свиней', 'копошиться', 'грязи', 'поисках', 'сумчатых', 'грибов', 'ковыряться', 'тине', 'собирая', 'брюхоногих', 'двустворчатых', 'народу', 'подсадившего', 'население', 'земного', 'шара', 'муть', 'именуемое', 'божоле', 'нуво', 'сыры', 'плесенью', 'квакающих', 'жаб', 'скачущих', 'болотам', 'менее', 'единожды', 'заглянув', 'мог', 'успокоиться', 'пока', 'вкусил', 'большую', 'часть', 'меню', 'буду', 'вдаваться', 'детали', 'вкусно', 'невкусно', 'приведу', 'турнирную', 'таблицу', 'коли', 'начали', 'хоккея', 'петух', 'запеченный', 'сливках', 'картофелем', 'грибами', 'почему', 'петух', 'курица', 'различать', 'научился', 'домашний', 'пирог', 'яблочный', 'горячий', 'тающий', 'говядина', 'бургунски', 'кускус', 'неожиданный', 'террин', 'печени', 'цыпленка', 'массивная', 'порция', 'утиная', 'ножка', 'конфи', 'картофельным', 'пюре', 'мягко', 'пенне', 'артишоками', 'чеснока', 'люблю', 'дышать', 'кого', 'случиться', 'казус', 'ростбиф', 'маринованным', 'луком', 'лягушка', 'вкус', 'смесь', 'птицы', 'рыбы', 'овощное', 'сате', 'гарнир', 'улитки', 'эскарго', 'просто', 'масле', 'нравятся', 'гребешки', 'спаржей', 'горячее', 'блюдо', 'весьма', 'диетично', 'гребешки', 'грушей', 'горячая', 'закуска', 'рядом', 'просто', 'листья', 'салата', 'теплый', 'салат', 'говядиной', 'рядом', 'просто', 'листья', 'салата', 'тирамису', 'качественный', 'уровень', 'соблюден', 'домашний', 'пирог', 'черничный', 'холодный', 'слишком', 'холодный', 'творог', 'основном', 'плюс', 'другие', 'приятности', 'приятный', 'грейпфрутовый', 'фреш', 'приятный', 'зеленый', 'чай', 'земляника', 'сливками', 'приятная', 'тертая', 'клубника', 'комплиментом', 'приятный', 'глинтвейн', 'зажали', 'интерьеру', 'комментария', 'краям', 'кабинки', 'диванчики', 'сварганить', 'уюта', 'одного', 'милого', 'столика', 'справа', 'входа', 'маловато', 'помывочная', 'белыми', 'дверями', 'почему', 'выбивается', 'стиля', 'остальном', 'типаж', 'музыка', 'мягкая', 'панели', 'сервис', 'улыбкой', 'шероховатости', 'пределах', 'нормы', 'незначительны', 'приборы', 'забыли', 'принести', 'глинтвейн', 'зажали', 'это', 'говорил', 'человек', 'хорошему', 'быстро', 'привыкает']\n"
     ]
    }
   ],
   "source": [
    "print(reviews_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск коллокаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('день', 'рождения'),\n",
       " ('молодой', 'человек'),\n",
       " ('очень', 'вкусно'),\n",
       " ('могу', 'сказать'),\n",
       " ('очень', 'понравилось'),\n",
       " ('живая', 'музыка'),\n",
       " ('отдельное', 'спасибо'),\n",
       " ('сих', 'пор'),\n",
       " ('крайней', 'мере'),\n",
       " ('самом', 'деле')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(reviews_tokenized)\n",
    "\n",
    "finder.nbest(bigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно взять ещё и триграммы, но занимает много времени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# finder_tri = nltk.collocations.TrigramCollocationFinder.from_documents(reviews_tokenized)\n",
    "# finder_tri.nbest(trigram_measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск топ 5 биграмм для расширенного списка оценочных слов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigramms_for_word(word):\n",
    "    word_filter = lambda *w: word not in w\n",
    "    \n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_documents(reviews_tokenized)\n",
    "    finder.apply_ngram_filter(word_filter)\n",
    "    \n",
    "    return finder.nbest(bigram_measures.likelihood_ratio, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_all_clean.txt', encoding = \"utf-8\") as file:\n",
    "    target_words = [line.strip() for line in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bigramms = collections.defaultdict()\n",
    "for word in target_words:\n",
    "    target_bigramms[word] = find_bigramms_for_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bigramms = dict(target_bigramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_all_clean_collocations.json', 'w', encoding = 'utf-8') as outfile:\n",
    "    json.dump(target_bigramms, outfile, \n",
    "              ensure_ascii=False, \n",
    "              indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
